{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training with TensorFlow in Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # For Jupyter notebooks, ensures plots display directly inline\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  # For Jupyter notebooks, ensures plots display directly inline\n",
    "\n",
    "# Standard library imports\n",
    "import os  # For interacting with the file system\n",
    "import tarfile  # For extracting .tar.gz files\n",
    "import urllib  # For downloading files from the web\n",
    "import shutil  # High-level file operations (copying, moving, deleting)\n",
    "import json  # For working with JSON data format\n",
    "import random  # For generating random numbers (useful for shuffling data)\n",
    "\n",
    "# Numerical libraries\n",
    "import numpy as np  # The workhorse for numerical operations in Python\n",
    "import tensorflow as tf  # The deep learning framework we'll be using\n",
    "import sagemaker  # Amazon SageMaker SDK for machine learning tasks\n",
    "\n",
    "# Image processing library\n",
    "from PIL import Image  # Python Imaging Library (PIL) for image manipulation\n",
    "from matplotlib import pyplot as plt  # For plotting images and visualizations\n",
    "\n",
    "# URLs for the Oxford-IIIT Pet Dataset\n",
    "urls = ['http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz', 'http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz']\n",
    "\n",
    "print('Libraries imported')  # A simple confirmation message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(data_dir, download_dir):\n",
    "    \"\"\"Downloads and extracts the dataset from the provided URLs.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: The directory where the extracted data should be stored.\n",
    "        download_dir: The directory where the downloaded files will be temporarily saved.\n",
    "    \"\"\"\n",
    "    for url in urls:\n",
    "        target_file = url.split('/')[-1]  # Get the filename from the URL (e.g., 'images.tar.gz')\n",
    "        \n",
    "        if target_file not in os.listdir(download_dir):  # Check if already downloaded\n",
    "            print('Downloading', url)\n",
    "            urllib.request.urlretrieve(url, os.path.join(download_dir, target_file))  # Download file\n",
    "            tf = tarfile.open(os.path.join(download_dir, target_file))  # Open tar file\n",
    "            tf.extractall(data_dir)  # Extract to data_dir\n",
    "        else:\n",
    "            print('Already downloaded', url)\n",
    "\n",
    "def get_annotations(file_path, annotations={}):\n",
    "    \"\"\"Reads annotations file and extracts image names and labels.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the annotations file.\n",
    "        annotations: An optional dictionary to store annotations (image_name: label).\n",
    "\n",
    "    Returns:\n",
    "        The updated annotations dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        rows = f.read().splitlines()  # Read file and split into lines\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        image_name, _, _, _ = row.split(' ')  # Split line by space and take first 4 values\n",
    "        class_name = image_name.split('_')[:-1]  # Get class name by splitting on '_' and taking all but last\n",
    "        class_name = '_'.join(class_name)  # Join class name parts back together\n",
    "        image_name = image_name + '.jpg'  # Add .jpg extension to image name\n",
    "\n",
    "        # Determine label based on class name\n",
    "        annotations[image_name] = 'cat' if class_name[0] != class_name[0].lower() else 'dog'\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
      "Downloading http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')  # Create a directory named 'data' if it doesn't exist\n",
    "\n",
    "download_and_extract('data', '.')  # Download and extract data to the 'data' directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples 7349\n"
     ]
    }
   ],
   "source": [
    "annotations = get_annotations('data/annotations/trainval.txt')  # Get annotations from trainval set\n",
    "annotations = get_annotations('data/annotations/test.txt', annotations)  # Add annotations from test set\n",
    "\n",
    "total_count = len(annotations.keys())  # Calculate the total number of annotated images\n",
    "print('Total examples', total_count)  # Print the total count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Abyssinian_100.jpg', 'cat')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(annotations.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cat', 'dog']  # Define the classes for your image classification task\n",
    "sets = ['train', 'validation']  # Define the different sets of data (train and validation)\n",
    "root_dir = 'custom_data'  # Define the root directory for your custom dataset\n",
    "\n",
    "if not os.path.isdir(root_dir):\n",
    "    os.mkdir(root_dir)  # Create the root directory if it doesn't exist\n",
    "\n",
    "for set_name in sets:  # Loop through the sets (train and validation)\n",
    "    if not os.path.isdir(os.path.join(root_dir, set_name)):\n",
    "        os.mkdir(os.path.join(root_dir, set_name))  # Create a directory for the current set\n",
    "\n",
    "    for class_name in classes:  # Loop through the classes (cat and dog)\n",
    "        folder = os.path.join(root_dir, set_name, class_name)  \n",
    "        if not os.path.isdir(folder):\n",
    "            os.mkdir(folder)  # Create a directory for the current class within the current set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the files to correct set/ class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, class_name in annotations.items():\n",
    "    # Iterate through each image filename (key) and its corresponding class label (value) in the `annotations` dictionary.\n",
    "    \n",
    "    target_set = 'validation' if random.randint(0, 99) < 20 else 'train'\n",
    "    # Assign the image to either 'validation' or 'train' set:\n",
    "    #   - `random.randint(0, 99)` generates a random integer between 0 (inclusive) and 99 (inclusive).\n",
    "    #   - If this random number is less than 20 (a 20% probability), the `target_set` is set to 'validation'.\n",
    "    #   - Otherwise (an 80% probability), `target_set` is set to 'train'.\n",
    "    #   - This effectively splits the data into roughly 80% for training and 20% for validation.\n",
    "\n",
    "    target_path = os.path.join(root_dir, target_set, class_name, image)\n",
    "    # Construct the full destination path for the image:\n",
    "    #   - `root_dir`: The base directory where you're storing the custom dataset (e.g., 'custom_data').\n",
    "    #   - `target_set`: The set this image belongs to ('train' or 'validation').\n",
    "    #   - `class_name`: The class label of the image ('cat' or 'dog').\n",
    "    #   - `image`: The original filename of the image.\n",
    "    #   - The final path might look like 'custom_data/train/cat/Abyssinian_100.jpg'.\n",
    "\n",
    "    shutil.copy(os.path.join('data/images/', image), target_path)\n",
    "    # Copy the image file to its new location:\n",
    "    #   - `os.path.join('data/images/', image)`: Forms the source path of the image in the original dataset.\n",
    "    #   - `target_path`: The destination path calculated in the previous step.\n",
    "    #   - `shutil.copy()` is a function that efficiently copies the file, preserving its contents and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_data/train/cat has 2279 images\n",
      "custom_data/train/dog has 4748 images\n",
      "custom_data/validation/cat has 835 images\n",
      "custom_data/validation/dog has 1811 images\n",
      "{'train': 7027, 'validation': 2646}\n"
     ]
    }
   ],
   "source": [
    "sets_counts = {\n",
    "    'train': 0,\n",
    "    'validation': 0\n",
    "}  # Initialize a dictionary to store image counts for each set\n",
    "\n",
    "for set_name in sets:  # Loop through the sets (train and validation)\n",
    "    for class_name in classes:  # Loop through the classes (cat and dog)\n",
    "        path = os.path.join(root_dir, set_name, class_name)  # Construct the path to the current class folder\n",
    "        count = len(os.listdir(path))  # Count the number of images in the folder\n",
    "        print(path, 'has', count, 'images')  # Print the count for each folder\n",
    "        sets_counts[set_name] += count  # Update the total count for the current set\n",
    "\n",
    "print(sets_counts)  # Print the final counts for each set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script - Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used two methods to create train.py file - with open and append - inline magic\n",
    "\n",
    "\n",
    "#!pip install ipython \n",
    "\n",
    "#%%writefile train.py  # Magic command to create a Python file named 'train.py'\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import argparse  # For parsing command-line arguments\n",
    "#import os\n",
    "#import json\n",
    "\n",
    "#def create_model():\n",
    "   # \"\"\"Creates a MobileNetV2-based image classification model.\"\"\"\n",
    "\n",
    "    #model = tf.keras.models.Sequential([\n",
    "        #tf.keras.applications.mobilnet_v2.MobileNetV2(\n",
    "            #include_top=False,       # Exclude the original classifier\n",
    "            #pooling='avg',           # Use average pooling for feature extraction\n",
    "            #weights='imagenet',       # Load pre-trained weights on ImageNet\n",
    "            #input_shape=(128, 128, 3) # Resize input images to 128x128 with 3 color channels\n",
    "        #),\n",
    "        #tf.keras.layers.Dropout(0.5),      # Add dropout for regularization (prevents overfitting)\n",
    "        #tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    #])\n",
    "\n",
    "    #model.layers[0].trainable = False  # Freeze the pre-trained MobileNetV2 layers\n",
    "    #model.compile(\n",
    "        #loss='binary_crossentropy',  # Loss function for binary classification\n",
    "        #optimizer='adam',             # Optimization algorithm\n",
    "        #metrics=['accuracy', 'precision', 'recall']  # Metrics to track during training\n",
    "    #)\n",
    "\n",
    "    #return model\n",
    "    \n",
    "# train.py\n",
    "with open('train.py', 'w') as f:  # Open train.py in write mode\n",
    "    f.write(\"\"\"\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def build_model():\n",
    "    \\\"\\\"\\\"Constructs a MobileNetV2-based image classification model.\\\"\\\"\\\"\n",
    "\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(128, 128, 3),\n",
    "        include_top=False,       # Exclude the original classifier\n",
    "        weights='imagenet'       # Load pre-trained weights on ImageNet\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),  \n",
    "        tf.keras.layers.Dropout(0.5),              \n",
    "        tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script - Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "def create_data_generators(root_dir, batch_size):\n",
    "    \"\"\"Creates data generators for training and validation sets.\n",
    "\n",
    "    This function takes the root directory of your custom dataset (containing 'train' and 'validation' folders) and a batch size as input. It returns two Keras ImageDataGenerator instances: one for training and one for validation.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The base directory of your custom dataset (e.g., 'custom_data').\n",
    "        batch_size: The number of images to process in each batch during training.\n",
    "\n",
    "    Returns:\n",
    "        train_data_generator: A Keras ImageDataGenerator for the training set.\n",
    "        val_data_generator: A Keras ImageDataGenerator for the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=[0.8, 1.2],\n",
    "        rotation_range=20\n",
    "    ).flow_from_directory(\n",
    "        os.path.join(root_dir, 'train'),\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    # 1. Training Data Generator:\n",
    "    #   - ImageDataGenerator is used to load images from the 'train' directory.\n",
    "    #   - preprocessing_function: Applies MobileNetV2's preprocessing function (scaling, normalization) to each image.\n",
    "    #   - horizontal_flip=True: Randomly flips some images horizontally (data augmentation).\n",
    "    #   - zoom_range=[0.8, 1.2]: Randomly zooms images in or out (data augmentation).\n",
    "    #   - rotation_range=20: Randomly rotates images (data augmentation).\n",
    "    #   - flow_from_directory:\n",
    "    #       - Reads images from subfolders in the 'train' directory (each subfolder represents a class).\n",
    "    #       - Automatically labels the images based on their subfolder.\n",
    "    #       - Resizes all images to 128x128 pixels.\n",
    "    #       - Creates batches of 'batch_size' images for efficient training.\n",
    "    #       - Sets class_mode to 'binary' for cat vs. dog classification.\n",
    "\n",
    "\n",
    "    val_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    ).flow_from_directory(\n",
    "        os.path.join(root_dir, 'validation'),\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    # 2. Validation Data Generator:\n",
    "    #   - Similar to the training generator, but without data augmentation (to assess model performance on original images).\n",
    "    #   - Loads images from the 'validation' directory.\n",
    "    #   - Applies the same preprocessing as the training set.\n",
    "\n",
    "    return train_data_generator, val_data_generator \n",
    "    # Returns both generators for use in training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.py', 'a') as f:  # Append to train.py\n",
    "    f.write(\"\"\"\n",
    "def create_data_loaders(train_dir, val_dir, batch_size=32, image_size=(128, 128)):\n",
    "    \\\"\\\"\\\"Creates training and validation data loaders.\\\"\\\"\\\"\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'  # For binary classification\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# Add preprocessing and augmentation here if needed\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script - Putting it Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile -a train.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"Main function for training the model.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()  # Create an argument parser\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=3)            # Add argument for epochs (default: 3)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)       # Add argument for batch size (default: 16)\n",
    "    parser.add_argument('--steps', type=int, default=int(5865/16))  # Add argument for steps per epoch (default: calculated)\n",
    "    parser.add_argument('--val_steps', type=int, default=int(1484/16)) # Add argument for validation steps (default: calculated)\n",
    "\n",
    "    parser.add_argument('--model_dir', type=str)                 # Add argument for local model directory\n",
    "    parser.add_argument('--sm_model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))  # Add argument for SageMaker model directory\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))  # Add argument for training data directory\n",
    "\n",
    "    args, _ = parser.parse_known_args()    # Parse arguments and ignore unknowns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to existing train.py\n",
    "with open('train.py', 'a') as f:  # Append to train.py\n",
    "    f.write(\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \\\"\\\"\\\"Main function for training the model.\\\"\\\"\\\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=5)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--val', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_ds, val_ds = create_data_loaders(args.train, args.val, args.batch_size)\n",
    "\n",
    "    # Train the model:\n",
    "    model = build_model()\n",
    "    \n",
    "    # Callback to stop training when validation loss doesn't improve\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "        epochs=args.epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Save the model locally in SavedModel format for SageMaker:\n",
    "    model.save(os.path.join(args.model_dir, 'model')) \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to S3\n",
      "Uploaded to s3://imagedatatf/data\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()               # Create a SageMaker session\n",
    "role = sagemaker.get_execution_role()     # Get the IAM role for SageMaker\n",
    "bucket_name = 'imagedatatf'              # Name of your S3 bucket\n",
    "\n",
    "print('Uploading data to S3')\n",
    "s3_data_path = sess.upload_data(path=root_dir, bucket=bucket_name, key_prefix='data')  # Corrected typo: get_execution_role\n",
    "print('Uploaded to', s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with TensorFlow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Replace 'your-iam-role-arn' with your actual IAM role ARN\n",
    "role = sagemaker.get_execution_role()     \n",
    "\n",
    "pets_estimator = TensorFlow(\n",
    "    entry_point='train.py',      \n",
    "    role=role,                     \n",
    "    instance_type='ml.p3.2xlarge',   # Or choose another appropriate instance type\n",
    "    instance_count=1,             \n",
    "    framework_version='2.12',     \n",
    "    py_version='py310',            # Specify Python version\n",
    "    output_path='s3://imagedatatf'  # Bucket name directly included\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit and train the model \n",
    "\n",
    "pets_estimator.fit(s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pets_predictor = pets_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "print('\\nModel is deployed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dir = 'custom_data/validation/cat/'\n",
    "cat_images = [os.path.join(cat_dir, x) for x in os.listdir(cat_dir)]\n",
    "print(cat_images[0])\n",
    "\n",
    "dog_dir = 'custom_data/validation/dog/'\n",
    "dog_images = [os.path.join(dog_dir, x) for x in os.listdir(dog_dir)]\n",
    "print(dog_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(128, 128))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    results = pets_predictor.predict(img)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = cat_images[0]\n",
    "results = get_pred(image_path)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = int(np.squeeze(results['predictions']) > 0.5)\n",
    "print('Predicted class_id:', class_id, 'with class_name:', classes[class_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(pets_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
